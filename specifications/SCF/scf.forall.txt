//===----------------------------------------------------------------------===//
// ForallOp
//===----------------------------------------------------------------------===//

def ForallOp : SCF_Op<"forall", [
       AttrSizedOperandSegments,
       AutomaticAllocationScope,
       DeclareOpInterfaceMethods<LoopLikeOpInterface,
          ["getInitsMutable", "getRegionIterArgs", "getLoopInductionVars", 
           "getLoopLowerBounds", "getLoopUpperBounds", "getLoopSteps",
           "promoteIfSingleIteration", "yieldTiledValuesAndReplace"]>,
       RecursiveMemoryEffects,
       SingleBlockImplicitTerminator<"scf::InParallelOp">,
       DeclareOpInterfaceMethods<RegionBranchOpInterface>,
       DestinationStyleOpInterface,
       HasParallelRegion
     ]> {
  let summary = "evaluate a block multiple times in parallel";
  let description = [{
    `scf.forall` is a target-independent multi-dimensional parallel
    region application operation. It has exactly one block that represents the
    parallel body and it takes index operands that specify lower bounds, upper
    bounds and steps.

    The op also takes a variadic number of tensor operands (`shared_outs`).
    The future buffers corresponding to these tensors are shared among all
    threads. Shared tensors should be accessed via their corresponding block
    arguments. If multiple threads write to a shared buffer in a racy
    fashion, these writes will execute in some unspecified order. Tensors that
    are not shared can be used inside the body (i.e., the op is not isolated
    from above); however, if a use of such a tensor bufferizes to a memory
    write, the tensor is privatized, i.e., a thread-local copy of the tensor is
    used. This ensures that memory side effects of a thread are not visible to
    other threads (or in the parent body), apart from explicitly shared tensors.

    The name "thread" conveys the fact that the parallel execution is mapped
    (i.e. distributed) to a set of virtual threads of execution, one function
    application per thread. Further lowerings are responsible for specifying
    how this is materialized on concrete hardware resources.

    An optional `mapping` is an attribute array that specifies processing units
    with their dimension, how it remaps 1-1 to a set of concrete processing
    element resources (e.g. a CUDA grid dimension or a level of concrete nested
    async parallelism). It is expressed via any attribute that implements the
    device mapping interface. It is the reponsibility of the lowering mechanism
    to interpret the `mapping` attributes in the context of the concrete target
    the op is lowered to, or to ignore it when the specification is ill-formed
    or unsupported for a particular target.

    The only allowed terminator is `scf.forall.in_parallel`.
    `scf.forall` returns one value per `shared_out` operand. The
    actions of the `scf.forall.in_parallel` terminators specify how to combine the
    partial results of all parallel invocations into a full value, in some
    unspecified order. The "destination" of each such op must be a `shared_out`
    block argument of the `scf.forall` op.

    The actions involved in constructing the return values are further described
    by `tensor.parallel_insert_slice`.

    `scf.forall` acts as an implicit synchronization point.

    When the parallel function body has side effects, their order is unspecified
    across threads.

    `scf.forall` can be printed in two different ways depending on
    whether the loop is normalized or not. The loop is 'normalized' when all
    lower bounds are equal to zero and steps are equal to one. In that case,
    `lowerBound` and `step` operands will be omitted during printing.

    Normalized loop example:

    ```mlir
    //
    // Sequential context.
    //
    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
        (%num_threads_1, %numthread_id_2) shared_outs(%o1 = %C, %o2 = %pointwise)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context, each thread with id = (%thread_id_1, %thread_id_2)
      // runs its version of the code.
      //
      %sA = tensor.extract_slice %A[f((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sB = tensor.extract_slice %B[g((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sC = tensor.extract_slice %o1[h((%thread_id_1, %thread_id_2))]:
        tensor<?x?xT> to tensor<?x?xT>
      %sD = linalg.matmul
        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)
        outs(%sC : tensor<?x?xT>)

      %spointwise = subtensor %o2[i((%thread_id_1, %thread_id_2))]:
        tensor<?xT> to tensor<?xT>
      %sE = linalg.add ins(%spointwise : tensor<?xT>) outs(%sD : tensor<?xT>)

      scf.forall.in_parallel {
        tensor.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
          tensor<?x?xT> into tensor<?x?xT>

        tensor.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
          tensor<?xT> into tensor<?xT>
      }
    }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Loop with loop bounds example:

    ```mlir
    //
    // Sequential context.
    //
    %pointwise = scf.forall (%i, %j) = (0, 0) to (%dim1, %dim2)
      step (%tileSize1, %tileSize2) shared_outs(%o1 = %out)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context.
      //
      %sA = tensor.extract_slice %A[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>
      %sB = tensor.extract_slice %B[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>
      %sC = tensor.extract_slice %o[%i, %j][%tileSize1, %tileSize2][1, 1]
        : tensor<?x?xT> to tensor<?x?xT>

      %add = linalg.map {"arith.addf"}
        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)
        outs(%sC : tensor<?x?xT>)

      scf.forall.in_parallel {
        tensor.parallel_insert_slice %add into
          %o[%i, %j][%tileSize1, %tileSize2][1, 1]
          : tensor<?x?xT> into tensor<?x?xT>
      }
    }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Example with mapping attribute:

    ```mlir
    //
    // Sequential context. Here `mapping` is expressed as GPU thread mapping
    // attributes
    //
    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
        (%num_threads_1, %numthread_id_2) shared_outs(...)
      -> (tensor<?x?xT>, tensor<?xT>) {
      //
      // Parallel context, each thread with id = **(%thread_id_2, %thread_id_1)**
      // runs its version of the code.
      //
       scf.forall.in_parallel {
         ...
      }
    } { mapping = [#gpu.thread<y>, #gpu.thread<x>] }
    // Implicit synchronization point.
    // Sequential context.
    //
    ```

    Example with privatized tensors:

    ```mlir
    %t0 = ...
    %t1 = ...
    %r = scf.forall ... shared_outs(%o = t0) -> tensor<?xf32> {
      // %t0 and %t1 are privatized. %t0 is definitely copied for each thread
      // because the scf.forall op's %t0 use bufferizes to a memory
      // write. In the absence of other conflicts, %t1 is copied only if there
      // are uses of %t1 in the body that bufferize to a memory read and to a
      // memory write.
      "some_use"(%t0)
      "some_use"(%t1)
    }
    ```
  }];
  let arguments = (ins
    Variadic<Index>:$dynamicLowerBound,
    Variadic<Index>:$dynamicUpperBound,
    Variadic<Index>:$dynamicStep,
    DenseI64ArrayAttr:$staticLowerBound,
    DenseI64ArrayAttr:$staticUpperBound,
    DenseI64ArrayAttr:$staticStep,
    Variadic<AnyRankedTensor>:$outputs,
    OptionalAttr<DeviceMappingArrayAttr>:$mapping);

  let results = (outs Variadic<AnyType>:$results);
  let regions = (region SizedRegion<1>:$region);

  let hasCanonicalizer = 1;
  let hasCustomAssemblyFormat = 1;
  let hasVerifier = 1;

  // The default builder does not add the proper body BBargs, roll our own.
  let skipDefaultBuilders = 1;
  let builders = [
    // Builder that takes loop bounds.
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$lbs,
       "ArrayRef<OpFoldResult>":$ubs, "ArrayRef<OpFoldResult>":$steps,
       "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
       CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
            "nullptr"> :$bodyBuilderFn)>,

    // Builder for normalized loop that takes only upper bounds.
    OpBuilder<(ins "ArrayRef<OpFoldResult>":$ubs,
       "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
       CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
            "nullptr"> :$bodyBuilderFn)>,
  ];

  let extraClassDeclaration = [{
    /// Get induction variables.
    SmallVector<Value> getInductionVars() {
      std::optional<SmallVector<Value>> maybeInductionVars = getLoopInductionVars();
      assert(maybeInductionVars.has_value() && "expected values");
      return *maybeInductionVars;
    }
    /// Get lower bounds as OpFoldResult.
    SmallVector<OpFoldResult> getMixedLowerBound() {
      std::optional<SmallVector<OpFoldResult>> maybeLowerBounds = getLoopLowerBounds();
      assert(maybeLowerBounds.has_value() && "expected values");
      return *maybeLowerBounds;
    }

    /// Get upper bounds as OpFoldResult.
    SmallVector<OpFoldResult> getMixedUpperBound() {
      std::optional<SmallVector<OpFoldResult>> maybeUpperBounds = getLoopUpperBounds();
      assert(maybeUpperBounds.has_value() && "expected values");
      return *maybeUpperBounds;
    }

    /// Get steps as OpFoldResult.
    SmallVector<OpFoldResult> getMixedStep() {
      std::optional<SmallVector<OpFoldResult>> maybeSteps = getLoopSteps();
      assert(maybeSteps.has_value() && "expected values");
      return *maybeSteps;
    }

    /// Get lower bounds as values.
    SmallVector<Value> getLowerBound(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedLowerBound());
    }

    /// Get upper bounds as values.
    SmallVector<Value> getUpperBound(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedUpperBound());
    }

    /// Get steps as values.
    SmallVector<Value> getStep(OpBuilder &b) {
      return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedStep());
    }

    int64_t getRank() { return getStaticLowerBound().size(); }

    /// Number of operands controlling the loop: lbs, ubs, steps
    unsigned getNumControlOperands() { return 3 * getRank(); }

    /// Number of dynamic operands controlling the loop: lbs, ubs, steps
    unsigned getNumDynamicControlOperands() {
      return getODSOperandIndexAndLength(3).first;
    }

    OpResult getTiedOpResult(OpOperand *opOperand) {
      assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
             "invalid operand");
      return getOperation()->getOpResult(
          opOperand->getOperandNumber() - getNumDynamicControlOperands());
    }

    /// Return the num_threads operand that is tied to the given thread id
    /// block argument.
    OpOperand *getTiedOpOperand(BlockArgument bbArg) {
      assert(bbArg.getArgNumber() >= getRank() && "invalid bbArg");

      return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
                                           bbArg.getArgNumber() - getRank());
    }

    /// Return the shared_outs operand that is tied to the given OpResult.
    OpOperand *getTiedOpOperand(OpResult opResult) {
      assert(opResult.getDefiningOp() == getOperation() && "invalid OpResult");
      return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
                                           opResult.getResultNumber());
    }

    BlockArgument getTiedBlockArgument(OpOperand *opOperand) {
      assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
             "invalid operand");

      return getBody()->getArgument(opOperand->getOperandNumber() -
                                    getNumDynamicControlOperands() + getRank());
    }

    ::mlir::Value getInductionVar(int64_t idx) {
      return getInductionVars()[idx];
    }

    ::mlir::Block::BlockArgListType getRegionOutArgs() {
      return getBody()->getArguments().drop_front(getRank());
    }

    /// Checks if the lbs are zeros and steps are ones.
    bool isNormalized();

    // The ensureTerminator method generated by SingleBlockImplicitTerminator is
    // unaware of the fact that our terminator also needs a region to be
    // well-formed. We override it here to ensure that we do the right thing.
    static void ensureTerminator(Region & region, OpBuilder & builder,
                                 Location loc);

    InParallelOp getTerminator();

    // Declare the shared_outs as inits/outs to DestinationStyleOpInterface.
    MutableOperandRange getDpsInitsMutable() { return getOutputsMutable(); }

    /// Returns operations within scf.forall.in_parallel whose destination
    /// operand is the block argument `bbArg`.
    SmallVector<Operation*> getCombiningOps(BlockArgument bbArg);
  }];
}

